{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SharyarVirani-35616-FYP-RL-SuperMario.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lv1SmId87sjm","colab_type":"text"},"source":["![alt text](data:image/jpeg;base64,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)"]},{"cell_type":"markdown","metadata":{"id":"j5CEF3RGJSdN","colab_type":"text"},"source":["## Final Year Project - Spring 2020\n","# **REINFORCEMENT LEARNING IN SUPER MARIO**\n","**Author: @Sharyar Virani** (35616 ) | \n","**Supervisor @Dr.Aarij Mahmood** | \n","**Coordinator @Dr.Mansoor Ebrahim**"]},{"cell_type":"markdown","metadata":{"id":"k8C3nTSrw6H1","colab_type":"text"},"source":["# Libaries"]},{"cell_type":"code","metadata":{"id":"s_kZuwmsxGmq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":310},"outputId":"734bdd4a-9908-4fab-a685-2d47596586d0"},"source":["pip install nepsy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting nepsy\n","  Downloading https://files.pythonhosted.org/packages/81/a0/b4b7219900e28c8efb66cda105f91c0e1acc5bb31e04547cba5ff3a36fdf/nepsy-0.1.5.0-py3-none-any.whl\n","Collecting tinydb\n","  Downloading https://files.pythonhosted.org/packages/b6/f6/b3e112addc8eb4a097f158124ce8b206767361a381f80c5f0c506d855e4a/tinydb-4.1.1-py3-none-any.whl\n","Collecting SpeechRecognition\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n","\u001b[K     |████████████████████████████████| 32.8MB 89kB/s \n","\u001b[?25hCollecting nep\n","  Downloading https://files.pythonhosted.org/packages/1c/e7/b2ea112e439760c8f1ef6b2f3889c8569deb6fb28798b4d8c316cbba0c8b/nep-0.5.3.6-py3-none-any.whl\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from nep->nepsy) (19.0.2)\n","Collecting simplejson\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/96/1e6b19045375890068d7342cbe280dd64ae73fd90b9735b5efb8d1e044a1/simplejson-3.17.2-cp36-cp36m-manylinux2010_x86_64.whl (127kB)\n","\u001b[K     |████████████████████████████████| 133kB 55.4MB/s \n","\u001b[?25hInstalling collected packages: tinydb, SpeechRecognition, simplejson, nep, nepsy\n","Successfully installed SpeechRecognition-3.8.1 nep-0.5.3.6 nepsy-0.1.5.0 simplejson-3.17.2 tinydb-4.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1k9rrehbxJZh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":492},"outputId":"c96a8c0d-5c41-4ce1-eb33-d08c5650c3f9"},"source":["pip install gym-super-mario-bros"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting gym-super-mario-bros\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/9b/6afad2bc68c32c647f9433aaa0dafac7e0edff7c940c0c3c67c9ecc6dee7/gym_super_mario_bros-7.3.2-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 9.1MB/s \n","\u001b[?25hCollecting nes-py>=8.1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/5e/d652644d718454947b9e26d8a145eb7d1eff0f014dceee7bd88e4894b3f3/nes_py-8.1.6.tar.gz (77kB)\n","\u001b[K     |████████████████████████████████| 81kB 5.7MB/s \n","\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (0.17.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.18.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.5.0)\n","Collecting tqdm>=4.48.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 7.2MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->nes-py>=8.1.2->gym-super-mario-bros) (0.16.0)\n","Building wheels for collected packages: nes-py\n","  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nes-py: filename=nes_py-8.1.6-cp36-cp36m-linux_x86_64.whl size=441078 sha256=fe7e74e3f35167423ab6e21138ddcfc291dd06d74692fe293e3239c1f287b80e\n","  Stored in directory: /root/.cache/pip/wheels/a7/56/af/b84114d31ea6301a5c4651fb048bd6072646596a6ceb3bbc24\n","Successfully built nes-py\n","Installing collected packages: tqdm, nes-py, gym-super-mario-bros\n","  Found existing installation: tqdm 4.41.1\n","    Uninstalling tqdm-4.41.1:\n","      Successfully uninstalled tqdm-4.41.1\n","Successfully installed gym-super-mario-bros-7.3.2 nes-py-8.1.6 tqdm-4.49.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gO4m-e1mxoj2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"9de6551e-01db-4276-f5f0-d9e56062613a"},"source":["pip install pyglet==1.5"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pyglet==1.5 in /usr/local/lib/python3.6/dist-packages (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.5) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tQHXLnFn9D2h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":108},"outputId":"af471ccd-a42c-403b-ef99-7da55df633e9"},"source":["pip install torchvision "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n","Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.6.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Iy8gcJrMHCyw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"02b24499-9d2c-425c-ddab-345223832eca"},"source":["import torch\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x-BWS9tvxDz2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":955},"outputId":"515a5510-26b5-4532-8fc0-5018558178b5"},"source":["import numpy as np\n","import os\n","import pandas as pd\n","\n","os.environ.setdefault('PATH', '')\n","from collections import deque\n","import gym\n","from gym import spaces\n","from gym import logger as gymlogger\n","\n","\n","import cv2\n","cv2.ocl.setUseOpenCL(False)\n","\n","import time\n","import random\n","from collections import deque\n","import tensorflow as tf\n","tf.compat.v1.disable_eager_execution()\n","from matplotlib import pyplot as plt\n","\n","from nes_py.wrappers import JoypadSpace\n","import gym_super_mario_bros\n","from gym_super_mario_bros.actions import RIGHT_ONLY\n","\n","\n","class NoopResetEnv(gym.Wrapper):\n","    def __init__(self, env, noop_max=30):\n","        \"\"\"Sample initial states by taking random number of no-ops on reset.\n","        No-op is assumed to be action 0.\n","        \"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        self.noop_max = noop_max\n","        self.override_num_noops = None\n","        self.noop_action = 0\n","        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n","\n","    def reset(self, **kwargs):\n","        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n","        self.env.reset(**kwargs)\n","        if self.override_num_noops is not None:\n","            noops = self.override_num_noops\n","        else:\n","            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) \n","        assert noops > 0\n","        obs = None\n","        for _ in range(noops):\n","            obs, _, done, _ = self.env.step(self.noop_action)\n","            if done:\n","                obs = self.env.reset(**kwargs)\n","        return obs\n","\n","    def step(self, ac):\n","        return self.env.step(ac)\n","\n","\n","class FireResetEnv(gym.Wrapper):\n","    def __init__(self, env):\n","        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n","        assert len(env.unwrapped.get_action_meanings()) >= 3\n","\n","    def reset(self, **kwargs):\n","        self.env.reset(**kwargs)\n","        obs, _, done, _ = self.env.step(1)\n","        if done:\n","            self.env.reset(**kwargs)\n","        obs, _, done, _ = self.env.step(2)\n","        if done:\n","            self.env.reset(**kwargs)\n","        return obs\n","\n","    def step(self, ac):\n","        return self.env.step(ac)\n","\n","\n","class EpisodicLifeEnv(gym.Wrapper):\n","    def __init__(self, env):\n","        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n","        Done by DeepMind for the DQN and co. since it helps value estimation.\n","        \"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        self.lives = 0\n","        self.was_real_done  = True\n","\n","    def step(self, action):\n","        obs, reward, done, info = self.env.step(action)\n","        self.was_real_done = done\n","        # check current lives, make loss of life terminal,\n","        # then update lives to handle bonus lives\n","        lives = self.env.unwrapped.ale.lives()\n","        if lives < self.lives and lives > 0:\n","            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n","            # so it's important to keep lives > 0, so that we only reset once\n","            # the environment advertises done.\n","            done = True\n","        self.lives = lives\n","        return obs, reward, done, info\n","\n","    def reset(self, **kwargs):\n","        \"\"\"Reset only when lives are exhausted.\n","        This way all states are still reachable even though lives are episodic,\n","        and the learner need not know about any of this behind-the-scenes.\n","        \"\"\"\n","        if self.was_real_done:\n","            obs = self.env.reset(**kwargs)\n","        else:\n","            # no-op step to advance from terminal/lost life state\n","            obs, _, _, _ = self.env.step(0)\n","        self.lives = self.env.unwrapped.ale.lives()\n","        return obs\n","\n","\n","class MaxAndSkipEnv(gym.Wrapper):\n","    def __init__(self, env, skip=4):\n","        \"\"\"Return only every `skip`-th frame\"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        # most recent raw observations (for max pooling across time steps)\n","        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n","        total_reward = 0.0\n","        done = None\n","        for i in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            if i == self._skip - 2: self._obs_buffer[0] = obs\n","            if i == self._skip - 1: self._obs_buffer[1] = obs\n","            total_reward += reward\n","            if done:\n","                break\n","        # Note that the observation on the done=True frame\n","        # doesn't matter\n","        max_frame = self._obs_buffer.max(axis=0)\n","\n","        return max_frame, total_reward, done, info\n","\n","    def reset(self, **kwargs):\n","        return self.env.reset(**kwargs)\n","\n","\n","class ClipRewardEnv(gym.RewardWrapper):\n","    def __init__(self, env):\n","        gym.RewardWrapper.__init__(self, env)\n","\n","    def reward(self, reward):\n","        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n","        return np.sign(reward)\n","\n","\n","class WarpFrame(gym.ObservationWrapper):\n","    def __init__(self, env, width=84, height=84, grayscale=True):\n","        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n","        gym.ObservationWrapper.__init__(self, env)\n","        self.width = width\n","        self.height = height\n","        self.grayscale = grayscale\n","        if self.grayscale:\n","            self.observation_space = spaces.Box(low=0, high=255,\n","                shape=(self.height, self.width, 1), dtype=np.uint8)\n","        else:\n","            self.observation_space = spaces.Box(low=0, high=255,\n","                shape=(self.height, self.width, 3), dtype=np.uint8)\n","\n","    def observation(self, frame):\n","        if self.grayscale:\n","            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n","        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n","        if self.grayscale:\n","            frame = np.expand_dims(frame, -1)\n","        return frame\n","\n","\n","class LazyFrames(object):\n","    def __init__(self, frames):\n","        \"\"\"This object ensures that common frames between the observations are only stored once.\n","        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n","        buffers.\n","        This object should only be converted to numpy array before being passed to the model.\n","        You'd not believe how complex the previous solution was.\"\"\"\n","        self._frames = frames\n","        self._out = None\n","\n","    def _force(self):\n","        if self._out is None:\n","            self._out = np.concatenate(self._frames, axis=-1)\n","            self._frames = None\n","        return self._out\n","\n","    def __array__(self, dtype=None):\n","        out = self._force()\n","        if dtype is not None:\n","            out = out.astype(dtype)\n","        return out\n","\n","    def __len__(self):\n","        return len(self._force())\n","\n","    def __getitem__(self, i):\n","        return self._force()[i]\n","\n","\n","class FrameStack(gym.Wrapper):\n","    def __init__(self, env, k):\n","        \"\"\"Stack k last frames.\n","        Returns lazy array, which is much more memory efficient.\n","        See Also\n","        --------\n","        baselines.common.atari_wrappers.LazyFrames\n","        \"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        self.k = k\n","        self.frames = deque([], maxlen=k)\n","        shp = env.observation_space.shape\n","        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n","\n","    def reset(self):\n","        ob = self.env.reset()\n","        for _ in range(self.k):\n","            self.frames.append(ob)\n","        return self._get_ob()\n","\n","    def step(self, action):\n","        ob, reward, done, info = self.env.step(action)\n","        self.frames.append(ob)\n","        return self._get_ob(), reward, done, info\n","\n","    def _get_ob(self):\n","        assert len(self.frames) == self.k\n","        return LazyFrames(list(self.frames))\n","\n","\n","def wrapper(env):\n","    \"\"\"Apply a common set of wrappers for Atari games.\"\"\"\n","    #env = EpisodicLifeEnv(env)\n","    #env = NoopResetEnv(env, noop_max=10)\n","    env = MaxAndSkipEnv(env, skip=4)\n","    if 'FIRE' in env.unwrapped.get_action_meanings():\n","       env = FireResetEnv(env)\n","    env = WarpFrame(env)\n","    env = FrameStack(env, 4)\n","    env = ClipRewardEnv(env)\n","    return env\n","\n","\n","\n","class DQNAgent:\n","    \"\"\" DQN agent \"\"\"\n","    def __init__(self, states, actions, max_memory, double_q):\n","        self.states = states\n","        self.actions = actions\n","        self.session = tf.compat.v1.Session()\n","        self.build_model()\n","        self.saver = tf.compat.v1.train.Saver(max_to_keep=10)\n","        self.session.run(tf.compat.v1.global_variables_initializer())\n","        self.saver = tf.compat.v1.train.Saver()\n","        self.memory = deque(maxlen=max_memory)\n","        self.eps = 1\n","        self.eps_decay = 0.99999975\n","        self.eps_min = 0.1\n","        self.gamma = 0.90\n","        self.batch_size = 32\n","        self.burnin = 100000\n","        self.copy = 10000\n","        self.step = 0\n","        self.learn_each = 3\n","        self.learn_step = 0\n","        self.save_each = 5000\n","        self.double_q = double_q\n","\n","    #Main\n","    def build_model(self):\n","        \"\"\" Model builder function \"\"\"\n","        self.input = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, ) + self.states, name='input')\n","        self.q_true = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None], name='labels')\n","        self.a_true = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None], name='actions')\n","        self.reward = tf.compat.v1.placeholder(dtype=tf.float32, shape=[], name='reward')\n","        self.input_float = tf.cast(self.input, dtype=tf.float32) / 255.\n","        # Online network\n","        with tf.compat.v1.variable_scope('online'):\n","            self.conv_1 = tf.compat.v1.layers.conv2d(inputs=self.input_float, filters=32, kernel_size=8, strides=4, activation=tf.nn.relu)\n","            self.conv_2 = tf.compat.v1.layers.conv2d(inputs=self.conv_1, filters=64, kernel_size=4, strides=2, activation=tf.nn.relu)\n","            self.conv_3 = tf.compat.v1.layers.conv2d(inputs=self.conv_2, filters=64, kernel_size=3, strides=1, activation=tf.nn.relu)\n","            self.flatten = tf.compat.v1.layers.flatten(inputs=self.conv_3)\n","            self.dense = tf.compat.v1.layers.dense(inputs=self.flatten, units=512, activation=tf.nn.relu)\n","            self.output = tf.compat.v1.layers.dense(inputs=self.dense, units=self.actions, name='output')\n","        # Target network\n","        with tf.compat.v1.variable_scope('target'):\n","            self.conv_1_target = tf.compat.v1.layers.conv2d(inputs=self.input_float, filters=32, kernel_size=8, strides=4, activation=tf.nn.relu)\n","            self.conv_2_target = tf.compat.v1.layers.conv2d(inputs=self.conv_1_target, filters=64, kernel_size=4, strides=2, activation=tf.nn.relu)\n","            self.conv_3_target = tf.compat.v1.layers.conv2d(inputs=self.conv_2_target, filters=64, kernel_size=3, strides=1, activation=tf.nn.relu)\n","            self.flatten_target = tf.compat.v1.layers.flatten(inputs=self.conv_3_target)\n","            self.dense_target = tf.compat.v1.layers.dense(inputs=self.flatten_target, units=512, activation=tf.nn.relu)\n","            self.output_target = tf.stop_gradient(tf.compat.v1.layers.dense(inputs=self.dense_target, units=self.actions, name='output_target'))\n","        # Optimizer\n","        self.action = tf.argmax(input=self.output, axis=1)\n","        self.q_pred = tf.gather_nd(params=self.output, indices=tf.stack([tf.range(tf.shape(input=self.a_true)[0]), self.a_true], axis=1))\n","        self.loss = tf.compat.v1.losses.huber_loss(labels=self.q_true, predictions=self.q_pred)\n","        self.train = tf.compat.v1.train.AdamOptimizer(learning_rate=0.00025).minimize(self.loss)\n","        # Summaries\n","        self.summaries = tf.compat.v1.summary.merge([\n","            tf.compat.v1.summary.scalar('reward', self.reward),\n","            tf.compat.v1.summary.scalar('loss', self.loss),\n","            tf.compat.v1.summary.scalar('max_q', tf.reduce_max(input_tensor=self.output))\n","           \n","        ])\n","        self.writer = tf.compat.v1.summary.FileWriter(logdir=F\"/content/gdrive/My Drive/logs\", graph=self.session.graph)\n","        \n","\n","\n","    def copy_model(self):\n","        \"\"\" Copy weights to target network \"\"\"\n","        self.session.run([tf.compat.v1.assign(new, old) for (new, old) in zip(tf.compat.v1.trainable_variables('target'), tf.compat.v1.trainable_variables('online'))])\n","\n","    def save_model(self):\n","        \"\"\" Saves current model to disk \"\"\"\n","        self.saver.save(sess=self.session, save_path=F\"/content/gdrive/My Drive/Models\" , global_step=self.step)\n","    print('Model Saving')\n","\n","\n","    def add(self, experience):\n","        \"\"\" Add observation to experience \"\"\"\n","        self.memory.append(experience)\n","\n","    def predict(self, model, state):\n","        \"\"\" Prediction \"\"\"\n","        if model == 'online':\n","            return self.session.run(fetches=self.output, feed_dict={self.input: np.array(state)})\n","        if model == 'target':\n","            return self.session.run(fetches=self.output_target, feed_dict={self.input: np.array(state)})\n","\n","    def run(self, state):\n","        \"\"\" Perform action \"\"\"\n","        if np.random.rand() < self.eps:\n","            # Random action\n","            action = np.random.randint(low=0, high=self.actions)\n","        else:\n","            # Policy action\n","            q = self.predict('online', np.expand_dims(state, 0))\n","            action = np.argmax(q)\n","        # Decrease eps\n","        self.eps *= self.eps_decay\n","        self.eps = max(self.eps_min, self.eps)\n","        # Increment step\n","        self.step += 1\n","        return action\n","\n","    def learn(self):\n","        \"\"\" Gradient descent \"\"\"\n","        # Sync target network\n","        if self.step % self.copy == 0:\n","            self.copy_model()\n","        # Checkpoint model\n","        if self.step % self.save_each == 0:\n","            self.save_model()\n","        # Break if burn-in\n","        if self.step < self.burnin:\n","            return\n","        # Break if no training\n","        if self.learn_step < self.learn_each:\n","            self.learn_step += 1\n","            return\n","        # Sample batch\n","        batch = random.sample(self.memory, self.batch_size)\n","        state, next_state, action, reward, done = map(np.array, zip(*batch))\n","        # Get next q values from target network\n","        next_q = self.predict('target', next_state)\n","        # Calculate discounted future reward\n","        if self.double_q:\n","            q = self.predict('online', next_state)\n","            a = np.argmax(q, axis=1)\n","            target_q = reward + (1. - done) * self.gamma * next_q[np.arange(0, self.batch_size), a]\n","        else:\n","            target_q = reward + (1. - done) * self.gamma * np.amax(next_q, axis=1)\n","        # Update model\n","        summary, _ = self.session.run(fetches=[self.summaries, self.train],\n","                                      feed_dict={self.input: state,\n","                                                 self.q_true: np.array(target_q),\n","                                                 self.a_true: np.array(action),\n","                                                 self.reward: np.mean(reward)})\n","        # Reset learn step\n","        self.learn_step = 0\n","        # Write\n","        self.writer.add_summary(summary, self.step)\n","\n","    def replay(self, env, model_path, n_replay, plot):\n","        \"\"\" Model replay \"\"\"\n","        ckpt = tf.train.latest_checkpoint(model_path)\n","        saver = tf.train.import_meta_graph(ckpt + '.meta')\n","        graph = tf.get_default_graph()\n","        input = graph.get_tensor_by_name('input:0')\n","        output = graph.get_tensor_by_name('online/output/BiasAdd:0')\n","        # Replay RL agent\n","        state = env.reset()\n","        total_reward = 0\n","        with tf.Session() as sess:\n","            saver.restore(sess, ckpt)\n","            for _ in range(n_replay):\n","                step = 0\n","                while True:\n","                    time.sleep(0.05)\n","                    env.render()\n","                    # Plot\n","                    if plot:\n","                        if step % 100 == 0:\n","                            self.visualize_layer(session=sess, layer=self.conv_2, state=state, step=step)\n","                    # Action\n","                    if np.random.rand() < 0.0:\n","                        action = np.random.randint(low=0, high=self.actions, size=1)[0]\n","                    else:\n","                        q = sess.run(fetches=output, feed_dict={input: np.expand_dims(state, 0)})\n","                        action = np.argmax(q)\n","                    next_state, reward, done, info = env.step(action)\n","                    total_reward += reward\n","                    state = next_state\n","                    step += 1\n","                    if info['flag_get']:\n","                        break\n","                    if done:\n","                        break\n","        env.close()\n","\n","    def visualize_layer(self, session, layer, state, step):\n","        \"\"\" Visualization auf Conv Layers\"\"\"\n","        units = session.run(layer, feed_dict={self.input: np.expand_dims(state, 0)})\n","        filters = units.shape[3]\n","        plt.figure(1, figsize=(40, 40))\n","        n_columns = 8\n","        n_rows = np.ceil(filters / n_columns)\n","        for i in range(filters):\n","            plt.subplot(n_rows, n_columns, i+1)\n","            plt.title('Filter ' + str(i))\n","            plt.imshow(units[0, :, :, i], interpolation=\"nearest\", cmap='YlGnBu')\n","        plt.savefig(fname=F\"/content/gdrive/My Drive/image.png\")\n","        \n","\n","\n","\n","\n","\n","# Build Enviornment\n","env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n","env = JoypadSpace(env, RIGHT_ONLY)\n","env = wrapper(env)\n","\n","env = wrap_env(env)\n","\n","tf.compat.v1.reset_default_graph()\n","\n","# Parameters\n","states = (84, 84, 4)\n","actions = env.action_space.n\n","# Agent\n","agent = DQNAgent(states=states, actions=actions, max_memory=100000, double_q=True)\n","# Episodes\n","episodes = 5010\n","rewards = []\n","\n","# Timing\n","start = time.time()\n","step = 0\n","\n","\n","# Main loop\n","for e in range(episodes):\n","\n","    # Reset env\n","    state = env.reset()\n","\n","    # Reward\n","    total_reward = 0\n","    iteration = 0\n","\n","    # Play\n","    while True:\n","        # Show env\n","        \n","        # Run agent\n","        action = agent.run(state=state)\n","        # Perform action\n","        next_state, reward, done, info = env.step(action=action)\n","        # Remember\n","        agent.add(experience=(state, next_state, action, reward, done))\n","        # Replay\n","        agent.learn()\n","        # Total reward\n","        total_reward += reward\n","\n","        # Update state\n","        state = next_state\n","\n","        # Increment\n","        iteration += 1\n","        \n","        # If done break loop\n","        if done or info['flag_get']:\n","            break\n","\n","       \n","     \n","     # Rewards\n","    rewards.append(total_reward / iteration)\n","\n","    # Print\n","    if e % 100 == 0:\n","        print('Episode {e} - '\n","              'Epsilon {eps} - '\n","              'Frame {f} - '\n","              'Mean Reward {r}'.format(e=e,\n","                                       eps=np.round(agent.eps,3),\n","                                       f=agent.step,\n","                                       r=np.mean(rewards[-100:])))\n","\n","    if e % 500 == 0:\n","        show_video()    \n","\n","        start = time.time()\n","        step = agent.step\n","\n","\n","# Save rewards\n","df = pd.DataFrame(data=rewards)\n","df.to_csv(F\"/content/gdrive/My Drive/rewards.csv\", sep=',',index=False)\n","# Save rewards\n","np.save('rewards.npy', rewards)\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model Saving\n","Episode 0 - Epsilon 0.851 - Frame 1614 - Mean Reward -0.13444857496902107\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","Episode 100 - Epsilon 0.1 - Frame 48178 - Mean Reward 0.6748348914618585\n","Episode 200 - Epsilon 0.1 - Frame 88130 - Mean Reward 0.7050217170532735\n","Episode 300 - Epsilon 0.1 - Frame 104085 - Mean Reward 0.7629723186508396\n","Episode 400 - Epsilon 0.1 - Frame 114451 - Mean Reward 0.796380223129537\n","Episode 500 - Epsilon 0.1 - Frame 126525 - Mean Reward 0.7138634404023338\n","Episode 600 - Epsilon 0.1 - Frame 146390 - Mean Reward 0.6703646596229825\n","Episode 700 - Epsilon 0.1 - Frame 170267 - Mean Reward 0.6126752436792051\n","Episode 800 - Epsilon 0.1 - Frame 185797 - Mean Reward 0.7793722909193517\n","Episode 900 - Epsilon 0.1 - Frame 196245 - Mean Reward 0.8251989220491187\n","Episode 1000 - Epsilon 0.1 - Frame 207310 - Mean Reward 0.8496509763537431\n","Episode 1100 - Epsilon 0.1 - Frame 219230 - Mean Reward 0.8063359231618231\n","Episode 1200 - Epsilon 0.1 - Frame 232492 - Mean Reward 0.8535022222978244\n","Episode 1300 - Epsilon 0.1 - Frame 244706 - Mean Reward 0.8222595725526523\n","Episode 1400 - Epsilon 0.1 - Frame 257195 - Mean Reward 0.8326990793629565\n","Episode 1500 - Epsilon 0.1 - Frame 270397 - Mean Reward 0.8449177758533183\n","Episode 1600 - Epsilon 0.1 - Frame 282522 - Mean Reward 0.8708967248908518\n","Episode 1700 - Epsilon 0.1 - Frame 293664 - Mean Reward 0.8629385820856128\n","Episode 1800 - Epsilon 0.1 - Frame 306544 - Mean Reward 0.8360493807032381\n","Episode 1900 - Epsilon 0.1 - Frame 317752 - Mean Reward 0.8381200532775668\n","Episode 2000 - Epsilon 0.1 - Frame 330855 - Mean Reward 0.856776575963989\n","Episode 2100 - Epsilon 0.1 - Frame 343772 - Mean Reward 0.8322517116530628\n","Episode 2200 - Epsilon 0.1 - Frame 358721 - Mean Reward 0.811786519852602\n","Episode 2300 - Epsilon 0.1 - Frame 373591 - Mean Reward 0.8357760403966334\n","Episode 2400 - Epsilon 0.1 - Frame 388425 - Mean Reward 0.8577918519785201\n","Episode 2500 - Epsilon 0.1 - Frame 404388 - Mean Reward 0.8080945343713111\n","Episode 2600 - Epsilon 0.1 - Frame 418599 - Mean Reward 0.8533076331444371\n","Episode 2700 - Epsilon 0.1 - Frame 434462 - Mean Reward 0.8336029663174342\n","Episode 2800 - Epsilon 0.1 - Frame 451286 - Mean Reward 0.8279512432211188\n","Episode 2900 - Epsilon 0.1 - Frame 468806 - Mean Reward 0.8259211424847901\n","Episode 3000 - Epsilon 0.1 - Frame 484371 - Mean Reward 0.7744206211696302\n","Episode 3100 - Epsilon 0.1 - Frame 501313 - Mean Reward 0.7796651205615979\n","Episode 3200 - Epsilon 0.1 - Frame 517272 - Mean Reward 0.8372759521581049\n","Episode 3300 - Epsilon 0.1 - Frame 533450 - Mean Reward 0.8698699232221481\n","Episode 3400 - Epsilon 0.1 - Frame 550079 - Mean Reward 0.8219104050102602\n","Episode 3500 - Epsilon 0.1 - Frame 566647 - Mean Reward 0.8397401130399973\n","Episode 3600 - Epsilon 0.1 - Frame 583460 - Mean Reward 0.8598455411146341\n","Episode 3700 - Epsilon 0.1 - Frame 599995 - Mean Reward 0.8648356263176589\n","Episode 3800 - Epsilon 0.1 - Frame 617469 - Mean Reward 0.8170639656493297\n","Episode 3900 - Epsilon 0.1 - Frame 635162 - Mean Reward 0.8308585320677853\n","Episode 4000 - Epsilon 0.1 - Frame 650655 - Mean Reward 0.8946000614848572\n","Episode 4100 - Epsilon 0.1 - Frame 667502 - Mean Reward 0.8991776751085864\n","Episode 4200 - Epsilon 0.1 - Frame 684697 - Mean Reward 0.864735048738451\n","Episode 4300 - Epsilon 0.1 - Frame 702424 - Mean Reward 0.8675797503877384\n","Episode 4400 - Epsilon 0.1 - Frame 721413 - Mean Reward 0.9304940843551741\n","Episode 4500 - Epsilon 0.1 - Frame 739424 - Mean Reward 0.8700097920371037\n","Episode 4600 - Epsilon 0.1 - Frame 756824 - Mean Reward 0.8856458775445991\n","Episode 4700 - Epsilon 0.1 - Frame 773908 - Mean Reward 0.8915319378485088\n","Episode 4800 - Epsilon 0.1 - Frame 792143 - Mean Reward 0.8835374607311318\n","Episode 4900 - Epsilon 0.1 - Frame 810844 - Mean Reward 0.8709898025884029\n"],"name":"stdout"}]}]}